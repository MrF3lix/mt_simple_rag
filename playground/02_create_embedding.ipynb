{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d9415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felix/MSE/03_projects/MT/06_rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb85c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAGRAPHS = '../data/kilt_wiki_small_paragraphs.jsonl'\n",
    "MODEL = 'jinaai/jina-embeddings-v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93888450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_json(PARAGRAPHS, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227ca335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/felix/.cache/huggingface/modules/transformers_modules/jinaai/xlm_hyphen_roberta_hyphen_flash_hyphen_implementation/2b6bc3f30750b3a9648fe9b63448c09920efe9be/rotary.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjinaai/jina-embeddings-v3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MSE/03_projects/MT/06_rag/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:327\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    309\u001b[39m has_modules = is_sentence_transformer_model(\n\u001b[32m    310\u001b[39m     model_name_or_path,\n\u001b[32m    311\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    315\u001b[39m )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    317\u001b[39m     has_modules\n\u001b[32m    318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_model_type(\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     == \u001b[38;5;28mself\u001b[39m._model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    326\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    340\u001b[39m         model_name_or_path,\n\u001b[32m    341\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         has_modules=has_modules,\n\u001b[32m    350\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MSE/03_projects/MT/06_rag/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:2286\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   2267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmodel_args\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mconfig_args\u001b[39m\u001b[33m\"\u001b[39m} <= \u001b[38;5;28mset\u001b[39m(signature.parameters):\n\u001b[32m   2268\u001b[39m     \u001b[38;5;66;03m# Load initialization arguments specific to Transformer-based modules. This includes\u001b[39;00m\n\u001b[32m   2269\u001b[39m     \u001b[38;5;66;03m# arguments for loading the model, tokenizer, and configuration, as well as any\u001b[39;00m\n\u001b[32m   2270\u001b[39m     \u001b[38;5;66;03m# additional module-specific keyword arguments.\u001b[39;00m\n\u001b[32m   2271\u001b[39m     common_transformer_init_kwargs = Transformer._load_init_kwargs(\n\u001b[32m   2272\u001b[39m         model_name_or_path,\n\u001b[32m   2273\u001b[39m         \u001b[38;5;66;03m# Loading-specific keyword arguments\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2284\u001b[39m         backend=\u001b[38;5;28mself\u001b[39m.backend,\n\u001b[32m   2285\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2286\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcommon_transformer_init_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2289\u001b[39m     \u001b[38;5;66;03m# Old modules that don't support the new loading method and don't seem Transformer-based\u001b[39;00m\n\u001b[32m   2290\u001b[39m     \u001b[38;5;66;03m# are loaded by downloading the full directories and calling .load() with the old style\u001b[39;00m\n\u001b[32m   2291\u001b[39m     \u001b[38;5;66;03m# (i.e. only a path to the local directory)\u001b[39;00m\n\u001b[32m   2292\u001b[39m     local_path = load_dir_path(\n\u001b[32m   2293\u001b[39m         model_name_or_path=model_name_or_path,\n\u001b[32m   2294\u001b[39m         subfolder=module_config[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   2298\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   2299\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/jina_hyphen_embeddings_hyphen_v3/f1944de8402dcd5f2b03f822a4bc22a7f2de2eb9/custom_st.py:82\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28mself\u001b[39m._adaptation_map = {\n\u001b[32m     77\u001b[39m     name: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m._lora_adaptations)\n\u001b[32m     78\u001b[39m }\n\u001b[32m     80\u001b[39m \u001b[38;5;28mself\u001b[39m.default_task = model_args.pop(\u001b[33m'\u001b[39m\u001b[33mdefault_task\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     85\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MSE/03_projects/MT/06_rag/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:586\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33madapter_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = adapter_kwargs\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m     model_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m     _ = hub_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    590\u001b[39m     \u001b[38;5;66;03m# This block handles the case where the user is loading a model with `trust_remote_code=True`\u001b[39;00m\n\u001b[32m    591\u001b[39m     \u001b[38;5;66;03m# but a library model exists with the same name. We don't want to override the autoclass\u001b[39;00m\n\u001b[32m    592\u001b[39m     \u001b[38;5;66;03m# mappings in this case, or all future loads of that model will be the remote code model.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MSE/03_projects/MT/06_rag/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:616\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m    604\u001b[39m final_module = get_cached_module_file(\n\u001b[32m    605\u001b[39m     repo_id,\n\u001b[32m    606\u001b[39m     module_file + \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    614\u001b[39m     repo_type=repo_type,\n\u001b[32m    615\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MSE/03_projects/MT/06_rag/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:299\u001b[39m, in \u001b[36mget_class_in_module\u001b[39m\u001b[34m(class_name, module_path, force_reload)\u001b[39m\n\u001b[32m    296\u001b[39m module_spec = importlib.util.spec_from_file_location(name, location=module_file)\n\u001b[32m    298\u001b[39m \u001b[38;5;66;03m# Hash the module file and all its relative imports to check if we need to reload it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m module_files: \u001b[38;5;28mlist\u001b[39m[Path] = [module_file] + \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mmap\u001b[39m(Path, \u001b[43mget_relative_import_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    300\u001b[39m module_hash: \u001b[38;5;28mstr\u001b[39m = hashlib.sha256(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mbytes\u001b[39m(f) + f.read_bytes() \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m module_files)).hexdigest()\n\u001b[32m    302\u001b[39m module: ModuleType\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MSE/03_projects/MT/06_rag/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:166\u001b[39m, in \u001b[36mget_relative_import_files\u001b[39m\u001b[34m(module_file)\u001b[39m\n\u001b[32m    164\u001b[39m new_imports = []\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files_to_check:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     new_imports.extend(\u001b[43mget_relative_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    168\u001b[39m module_path = Path(module_file).parent\n\u001b[32m    169\u001b[39m new_import_files = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(module_path\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39mm)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m new_imports]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MSE/03_projects/MT/06_rag/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:135\u001b[39m, in \u001b[36mget_relative_imports\u001b[39m\u001b[34m(module_file)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_relative_imports\u001b[39m(module_file: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike]) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m    Get the list of modules that are relatively imported in a module file.\u001b[39;00m\n\u001b[32m    128\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m \u001b[33;03m        `list[str]`: The list of relative imports in the module.\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    136\u001b[39m         content = f.read()\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# Imports of the form `import .xxx`\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/felix/.cache/huggingface/modules/transformers_modules/jinaai/xlm_hyphen_roberta_hyphen_flash_hyphen_implementation/2b6bc3f30750b3a9648fe9b63448c09920efe9be/rotary.py'"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5827891",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"text-matching\"\n",
    "embeddings = model.encode(\n",
    "    df_p['paragraph'],\n",
    "    task=task,\n",
    "    prompt_name=task,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03afe604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "06-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
